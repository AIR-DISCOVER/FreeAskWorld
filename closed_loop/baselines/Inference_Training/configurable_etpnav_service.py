#!/usr/bin/env python3
"""
å¯é…ç½®çš„ETPNavæ¨ç†æœåŠ¡
æ”¯æŒé€šè¿‡é…ç½®æ–‡ä»¶æˆ–å‚æ•°æŒ‡å®šæƒé‡è·¯å¾„ã€è¾“å‡ºè·¯å¾„ç­‰
"""
import multiprocessing as mp
import threading
import queue
import time
import numpy as np
import torch
import json
import cv2
from typing import Dict, Any, Optional, Tuple, Callable
import sys
import os
from dataclasses import dataclass
from enum import Enum
from PIL import Image
from datetime import datetime
import yaml


class InferenceStatus(Enum):
    """æ¨ç†çŠ¶æ€æšä¸¾"""
    PENDING = "PENDING"
    PROCESSING = "PROCESSING"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"


@dataclass
class InferenceResult:
    """æ¨ç†ç»“æœæ•°æ®ç±»"""
    request_id: str
    status: InferenceStatus
    content: Optional[Dict] = None
    error: Optional[str] = None
    start_time: float = 0.0
    end_time: float = 0.0

    @property
    def duration(self) -> float:
        """è®¡ç®—æ¨ç†è€—æ—¶"""
        return self.end_time - self.start_time


@dataclass
class ETPNavConfig:
    """ETPNavé…ç½®ç±»"""
    # æ¨¡å‹é…ç½®
    use_real_model: bool = True
    checkpoint_path: str = "data/logs/checkpoints/release_r2r/ckpt.iter12000.pth"
    config_path: str = "run_r2r/iter_train.yaml"
    device: str = "cuda"

    # è¾“å‡ºé…ç½®
    output_dir: str = "etpnav_outputs"
    save_results: bool = True
    result_format: str = "json"  # json, yaml, both

    # æ—¥å¿—é…ç½®
    log_level: str = "INFO"
    log_file: Optional[str] = None

    # æ€§èƒ½é…ç½®
    enable_callbacks: bool = True
    max_queue_size: int = 1000
    worker_timeout: float = 30.0

    @classmethod
    def from_file(cls, config_file: str) -> 'ETPNavConfig':
        """ä»é…ç½®æ–‡ä»¶åŠ è½½"""
        if config_file.endswith('.json'):
            with open(config_file, 'r') as f:
                data = json.load(f)
        elif config_file.endswith('.yaml') or config_file.endswith('.yml'):
            with open(config_file, 'r') as f:
                data = yaml.safe_load(f)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„é…ç½®æ–‡ä»¶æ ¼å¼: {config_file}")

        return cls(**data)

    def save(self, config_file: str):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        data = self.__dict__

        if config_file.endswith('.json'):
            with open(config_file, 'w') as f:
                json.dump(data, f, indent=2)
        elif config_file.endswith('.yaml') or config_file.endswith('.yml'):
            with open(config_file, 'w') as f:
                yaml.dump(data, f, default_flow_style=False)


class ConfigurableETPNavService:
    """å¯é…ç½®çš„ETPNavæ¨ç†æœåŠ¡"""

    def __init__(self, config: Optional[ETPNavConfig] = None, config_file: Optional[str] = None):
        """
        åˆå§‹åŒ–æœåŠ¡

        Args:
            config: ETPNavConfigå¯¹è±¡
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # åŠ è½½é…ç½®
        if config_file:
            self.config = ETPNavConfig.from_file(config_file)
        elif config:
            self.config = config
        else:
            self.config = ETPNavConfig()

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.config.output_dir, exist_ok=True)

        # è®¾ç½®æ—¥å¿—
        self._setup_logging()

        # é€šä¿¡é˜Ÿåˆ—
        self.request_queue = mp.Queue(maxsize=self.config.max_queue_size)
        self.completion_queue = mp.Queue(maxsize=self.config.max_queue_size)
        self.status_queue = mp.Queue(maxsize=self.config.max_queue_size)

        # è¿›ç¨‹ç®¡ç†
        self.etpnav_process = None
        self.running = False

        # å›è°ƒç®¡ç†
        self.callbacks = {
            'on_inference_start': [],
            'on_inference_complete': [],
            'on_inference_error': [],
            'on_status_change': []
        }

        # çŠ¶æ€è¿½è¸ª
        self.active_requests = {}

        # ç›‘å¬çº¿ç¨‹
        self.listener_thread = None

        # æ—¥å¿—
        self.logger.info(f"åˆå§‹åŒ–ETPNavæœåŠ¡")
        self.logger.info(f"é…ç½®: {self.config}")

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        import logging

        self.logger = logging.getLogger('ETPNavService')
        self.logger.setLevel(getattr(logging, self.config.log_level))

        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(getattr(logging, self.config.log_level))

        # æ ¼å¼åŒ–å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        console_handler.setFormatter(formatter)

        self.logger.addHandler(console_handler)

        # æ–‡ä»¶å¤„ç†å™¨
        if self.config.log_file:
            file_handler = logging.FileHandler(self.config.log_file)
            file_handler.setLevel(getattr(logging, self.config.log_level))
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)

    def register_callback(self, event: str, callback: Callable):
        """æ³¨å†Œå›è°ƒå‡½æ•°"""
        if event in self.callbacks:
            self.callbacks[event].append(callback)
            self.logger.info(f"å·²æ³¨å†Œå›è°ƒ: {event} -> {callback.__name__}")

    def start_service(self):
        """å¯åŠ¨ETPNavæœåŠ¡"""
        self.etpnav_process = mp.Process(
            target=self._etpnav_worker_process,
            args=(self.request_queue, self.completion_queue, self.status_queue, self.config)
        )
        self.etpnav_process.start()
        self.running = True

        # å¯åŠ¨ç›‘å¬çº¿ç¨‹
        if self.config.enable_callbacks:
            self.listener_thread = threading.Thread(target=self._listen_for_updates)
            self.listener_thread.daemon = True
            self.listener_thread.start()

        self.logger.info("ETPNavæœåŠ¡å·²å¯åŠ¨")
        self.logger.info(f"æ¨¡å‹æ¨¡å¼: {'çœŸå®ETPNav' if self.config.use_real_model else 'Fallback'}")
        self.logger.info(f"æƒé‡è·¯å¾„: {self.config.checkpoint_path}")
        self.logger.info(f"è¾“å‡ºç›®å½•: {self.config.output_dir}")

    def _listen_for_updates(self):
        """ç›‘å¬çŠ¶æ€æ›´æ–°å¹¶è§¦å‘å›è°ƒ"""
        while self.running:
            try:
                # æ£€æŸ¥çŠ¶æ€é˜Ÿåˆ—
                try:
                    status_update = self.status_queue.get_nowait()
                    self._handle_status_update(status_update)
                except queue.Empty:
                    pass

                # æ£€æŸ¥å®Œæˆé˜Ÿåˆ—
                try:
                    completion = self.completion_queue.get_nowait()
                    self._handle_completion(completion)
                except queue.Empty:
                    pass

                time.sleep(0.001)

            except Exception as e:
                self.logger.error(f"ç›‘å¬çº¿ç¨‹é”™è¯¯: {e}")

    def _handle_status_update(self, update: Dict):
        """å¤„ç†çŠ¶æ€æ›´æ–°"""
        request_id = update.get('request_id')
        status = update.get('status')

        if request_id in self.active_requests:
            self.active_requests[request_id]['status'] = status

        for callback in self.callbacks['on_status_change']:
            try:
                callback(request_id, status, update)
            except Exception as e:
                self.logger.error(f"çŠ¶æ€å›è°ƒé”™è¯¯: {e}")

    def _handle_completion(self, completion: Dict):
        """å¤„ç†æ¨ç†å®Œæˆ"""
        request_id = completion.get('request_id')

        result = InferenceResult(
            request_id=request_id,
            status=InferenceStatus.COMPLETED if completion.get('status') == 'SUCCESS' else InferenceStatus.FAILED,
            content=completion.get('content'),
            error=completion.get('error'),
            start_time=self.active_requests.get(request_id, {}).get('start_time', 0),
            end_time=completion.get('timestamp', time.time())
        )

        # ä¿å­˜ç»“æœ
        if self.config.save_results:
            self._save_result(result)

        # è§¦å‘å›è°ƒ
        if result.status == InferenceStatus.COMPLETED:
            for callback in self.callbacks['on_inference_complete']:
                try:
                    callback(result)
                except Exception as e:
                    self.logger.error(f"å®Œæˆå›è°ƒé”™è¯¯: {e}")
        else:
            for callback in self.callbacks['on_inference_error']:
                try:
                    callback(result)
                except Exception as e:
                    self.logger.error(f"é”™è¯¯å›è°ƒé”™è¯¯: {e}")

        if request_id in self.active_requests:
            self.active_requests[request_id]['result'] = result
            self.active_requests[request_id]['completed'] = True

    def _save_result(self, result: InferenceResult):
        """ä¿å­˜æ¨ç†ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = f"{result.request_id}_{timestamp}"

        result_data = {
            "request_id": result.request_id,
            "status": result.status.value,
            "duration": result.duration,
            "content": result.content,
            "error": result.error,
            "timestamp": time.time()
        }

        # ä¿å­˜JSON
        if self.config.result_format in ['json', 'both']:
            json_path = os.path.join(self.config.output_dir, f"{base_name}.json")
            with open(json_path, 'w') as f:
                json.dump(result_data, f, indent=2)
            self.logger.debug(f"ç»“æœå·²ä¿å­˜: {json_path}")

        # ä¿å­˜YAML
        if self.config.result_format in ['yaml', 'both']:
            yaml_path = os.path.join(self.config.output_dir, f"{base_name}.yaml")
            with open(yaml_path, 'w') as f:
                yaml.dump(result_data, f, default_flow_style=False)
            self.logger.debug(f"ç»“æœå·²ä¿å­˜: {yaml_path}")

    def send_inference_request(self, rgb_image: np.ndarray, depth_image: np.ndarray,
                               instruction: str, position: list, rotation: list,
                               priority: int = 0) -> str:
        """å‘é€æ¨ç†è¯·æ±‚"""
        request_id = f"etpnav_{int(time.time() * 1000000)}"
        start_time = time.time()

        self.active_requests[request_id] = {
            'start_time': start_time,
            'status': InferenceStatus.PENDING,
            'completed': False,
            'result': None
        }

        request = {
            "type": "INFERENCE_REQUEST",
            "request_id": request_id,
            "priority": priority,
            "data": {
                "rgb_image": rgb_image,
                "depth_image": depth_image,
                "instruction": instruction,
                "position": position,
                "rotation": rotation
            },
            "timestamp": start_time
        }

        self.request_queue.put(request)

        for callback in self.callbacks['on_inference_start']:
            try:
                callback(request_id, request['data'])
            except Exception as e:
                self.logger.error(f"å¼€å§‹å›è°ƒé”™è¯¯: {e}")

        self.logger.info(f"æ¨ç†è¯·æ±‚å·²å‘é€: {request_id}")

        return request_id

    def get_inference_result(self, request_id: str, timeout: float = 0.0) -> Optional[InferenceResult]:
        """è·å–æ¨ç†ç»“æœ"""
        if request_id not in self.active_requests:
            return None

        if timeout == 0:
            request_info = self.active_requests.get(request_id)
            return request_info.get('result') if request_info else None
        else:
            start_time = time.time()
            while time.time() - start_time < timeout:
                request_info = self.active_requests.get(request_id)
                if request_info and request_info.get('completed'):
                    return request_info.get('result')
                time.sleep(0.001)
            return None

    def stop_service(self):
        """åœæ­¢æœåŠ¡"""
        if self.running:
            self.running = False
            self.request_queue.put({"type": "STOP"})

            if self.listener_thread:
                self.listener_thread.join(timeout=1)

            if self.etpnav_process:
                self.etpnav_process.join(timeout=5)
                if self.etpnav_process.is_alive():
                    self.etpnav_process.terminate()

            self.logger.info("ETPNavæœåŠ¡å·²åœæ­¢")

    @staticmethod
    def _etpnav_worker_process(request_queue, completion_queue, status_queue, config: ETPNavConfig):
        # å¤šè¿›ç¨‹habitat.configä¿®è¡¥
        import sys
        import os
        current_dir = os.getcwd()
        if current_dir not in sys.path:
            sys.path.insert(0, current_dir)
            print(f"ğŸ”§ å·¥ä½œè¿›ç¨‹è·¯å¾„ä¿®è¡¥: {current_dir}")

        """ETPNavå·¥ä½œè¿›ç¨‹"""
        print(f"ğŸ¯ ETPNavå·¥ä½œè¿›ç¨‹å¯åŠ¨")
        print(f"   é…ç½®: {config.checkpoint_path}")

        # åˆå§‹åŒ–å·¥ä½œå™¨
        from real_etpnav_service import RealETPNavWorker

        # ä¿®æ”¹RealETPNavWorkerä»¥æ¥å—é…ç½®
        class ConfigurableETPNavWorker(RealETPNavWorker):
            def __init__(self, config: ETPNavConfig):
                self.config = config
                self.device = torch.device(config.device if torch.cuda.is_available() else 'cpu')
                self.use_real_model = config.use_real_model
                self.trainer = None
                self.model_loaded = False

                print(f"ğŸ–¥ï¸ è®¾å¤‡: {self.device}")

                if self.use_real_model:
                    if self.load_etpnav_model_with_config():
                        print("âœ… çœŸå®ETPNavæ¨¡å‹åŠ è½½æˆåŠŸ")
                        self.model_loaded = True
                    else:
                        print("âš ï¸ ETPNavæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æ™ºèƒ½fallbackæ¨¡å¼")
                        self.model_loaded = False
                else:
                    print("ğŸ“Œ ä½¿ç”¨Fallbackæ¨¡å¼")
                    self.model_loaded = False

            def load_etpnav_model_with_config(self):
                """ä½¿ç”¨é…ç½®åŠ è½½æ¨¡å‹"""
                print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {self.config.checkpoint_path}")
                print(f"ğŸ“„ ä½¿ç”¨é…ç½®æ–‡ä»¶: {self.config.config_path}")

                try:
                    # æ£€æŸ¥æƒé‡æ–‡ä»¶
                    if not os.path.exists(self.config.checkpoint_path):
                        print(f"âŒ æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {self.config.checkpoint_path}")
                        return False

                    # æ£€æŸ¥é…ç½®æ–‡ä»¶
                    if not os.path.exists(self.config.config_path):
                        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config.config_path}")
                        print(f"âš ï¸  å°†ä½¿ç”¨é»˜è®¤é…ç½®æ–‡ä»¶")

                    # è®¾ç½®è¦ä½¿ç”¨çš„é…ç½®è·¯å¾„
                    self.model_config_path = self.config.config_path
                    self.checkpoint_path = self.config.checkpoint_path

                    # ç»§ç»­åŸæœ‰çš„åŠ è½½é€»è¾‘
                    return self.load_etpnav_model()

                except Exception as e:
                    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    return False

        worker = ConfigurableETPNavWorker(config)

        # å¤„ç†è¯·æ±‚
        pending_requests = []

        while True:
            try:
                # æ”¶é›†æ–°è¯·æ±‚
                while True:
                    try:
                        request = request_queue.get_nowait()
                        if request.get("type") == "STOP":
                            print("ğŸ›‘ å·¥ä½œè¿›ç¨‹æ”¶åˆ°åœæ­¢ä¿¡å·")
                            return
                        pending_requests.append(request)
                    except queue.Empty:
                        break

                # æŒ‰ä¼˜å…ˆçº§æ’åº
                pending_requests.sort(key=lambda x: -x.get('priority', 0))

                # å¤„ç†è¯·æ±‚
                if pending_requests:
                    request = pending_requests.pop(0)

                    if request.get("type") == "INFERENCE_REQUEST":
                        request_id = request["request_id"]
                        data = request["data"]

                        status_queue.put({
                            'request_id': request_id,
                            'status': InferenceStatus.PROCESSING,
                            'timestamp': time.time()
                        })

                        print(f"ğŸ“¥ å¤„ç†è¯·æ±‚: {request_id}")

                        try:
                            result = worker.process_direct_data(
                                rgb_image=data["rgb_image"],
                                depth_image=data["depth_image"],
                                instruction=data["instruction"],
                                position=data["position"],
                                rotation=data["rotation"]
                            )

                            completion = {
                                "request_id": request_id,
                                "timestamp": time.time(),
                                "status": "SUCCESS",
                                "content": result
                            }

                            completion_queue.put(completion)
                            print(f"ğŸ“¤ æ¨ç†å®Œæˆ: {request_id}")

                        except Exception as e:
                            completion = {
                                "request_id": request_id,
                                "timestamp": time.time(),
                                "status": "FAILED",
                                "error": str(e)
                            }
                            completion_queue.put(completion)
                            print(f"âŒ æ¨ç†å¤±è´¥: {request_id} - {e}")

                time.sleep(0.001)

            except Exception as e:
                print(f"âŒ å·¥ä½œè¿›ç¨‹é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()


# ä¾¿æ·çš„å·¥å‚å‡½æ•°
def create_etpnav_service(config_file: Optional[str] = None, **kwargs) -> ConfigurableETPNavService:
    """
    åˆ›å»ºETPNavæœåŠ¡

    Args:
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        **kwargs: ç›´æ¥ä¼ é€’çš„é…ç½®å‚æ•°

    Returns:
        ConfigurableETPNavServiceå®ä¾‹
    """
    if config_file:
        return ConfigurableETPNavService(config_file=config_file)
    else:
        config = ETPNavConfig(**kwargs)
        return ConfigurableETPNavService(config=config)


# ç¤ºä¾‹é…ç½®æ–‡ä»¶ç”Ÿæˆ
def generate_example_config(output_file: str = "etpnav_config.yaml"):
    """ç”Ÿæˆç¤ºä¾‹é…ç½®æ–‡ä»¶"""
    config = ETPNavConfig()
    config.save(output_file)
    print(f"ç¤ºä¾‹é…ç½®æ–‡ä»¶å·²ç”Ÿæˆ: {output_file}")


if __name__ == "__main__":
    # ç”Ÿæˆç¤ºä¾‹é…ç½®
    generate_example_config()