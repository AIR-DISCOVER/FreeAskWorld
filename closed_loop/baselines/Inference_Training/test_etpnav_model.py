import sys
import os
import json
import torch
import torch.nn as nn
import logging
import argparse
import numpy as np
from pathlib import Path
import gzip
import time
import random

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ETPNavTester:
    """ETPNavæ¨¡å‹æµ‹è¯•å™¨ - åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°"""
    
    def __init__(self, checkpoint_path, device='cpu'):
        self.device = torch.device(device)
        self.checkpoint_path = checkpoint_path
        
        logger.info(f"ğŸ§ª ETPNavæ¨¡å‹æµ‹è¯•å™¨")
        logger.info(f"   è®¾å¤‡: {self.device}")
        logger.info(f"   æ¨¡å‹è·¯å¾„: {checkpoint_path}")
        
        # åŠ è½½checkpoint
        self.load_model_from_checkpoint()
    
    def load_model_from_checkpoint(self):
        """ä»checkpointåŠ è½½æ¨¡å‹"""
        try:
            logger.info("ğŸ“¥ åŠ è½½æ¨¡å‹checkpoint...")
            checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
            
            # è·å–é…ç½®ä¿¡æ¯
            self.config = checkpoint['config']
            self.vocab = checkpoint['vocab']
            self.vocab_size = len(self.vocab)
            self.pad_token_id = self.vocab['<pad>']
            self.best_metrics = checkpoint['best_metrics']
            
            logger.info(f"   è®­ç»ƒepoch: {checkpoint['epoch']}")
            logger.info(f"   è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
            logger.info(f"   æœ€ä½³val_unseen L2: {self.best_metrics['val_unseen_l2_error']:.4f}m")
            
            # é‡å»ºæ¨¡å‹æ¶æ„ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
            self.create_model_architecture()
            
            # åŠ è½½æ¨¡å‹æƒé‡
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.eval()
            
            logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e
    
    def create_model_architecture(self):
        """åˆ›å»ºæ¨¡å‹æ¶æ„ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰"""
        
        class ETPNavModel(nn.Module):
            def __init__(self, vocab_size, hidden_size=512):
                super().__init__()
                
                self.hidden_size = hidden_size
                self.vocab_size = vocab_size
                
                # æŒ‡ä»¤ç¼–ç å™¨
                self.instruction_embedding = nn.Embedding(vocab_size, 256, padding_idx=0)
                self.instruction_transformer = nn.TransformerEncoder(
                    nn.TransformerEncoderLayer(
                        d_model=256, 
                        nhead=8, 
                        dim_feedforward=512,
                        dropout=0.1,
                        batch_first=True
                    ),
                    num_layers=3
                )
                self.instruction_projection = nn.Linear(256, hidden_size)
                
                # è§†è§‰ç¼–ç å™¨
                self.visual_encoder = nn.Sequential(
                    nn.Conv2d(3, 64, 7, stride=2, padding=3),
                    nn.BatchNorm2d(64),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(3, stride=2, padding=1),
                    
                    self._make_resnet_block(64, 128),
                    self._make_resnet_block(128, 256), 
                    self._make_resnet_block(256, 512),
                    
                    nn.AdaptiveAvgPool2d((1, 1)),
                    nn.Flatten(),
                    nn.Linear(512, hidden_size)
                )
                
                # æ·±åº¦ç¼–ç å™¨
                self.depth_encoder = nn.Sequential(
                    nn.Conv2d(1, 64, 7, stride=2, padding=3),
                    nn.BatchNorm2d(64),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(3, stride=2, padding=1),
                    
                    self._make_resnet_block(64, 128),
                    self._make_resnet_block(128, 256),
                    self._make_resnet_block(256, 512),
                    
                    nn.AdaptiveAvgPool2d((1, 1)),
                    nn.Flatten(),
                    nn.Linear(512, hidden_size)
                )
                
                # è§†è§‰ç‰¹å¾èåˆ
                self.visual_fusion = nn.Sequential(
                    nn.Linear(hidden_size * 2, hidden_size),
                    nn.LayerNorm(hidden_size),
                    nn.ReLU(),
                    nn.Dropout(0.1)
                )
                
                # è·¨æ¨¡æ€Transformerèåˆ
                self.cross_modal_fusion = nn.TransformerEncoder(
                    nn.TransformerEncoderLayer(
                        d_model=hidden_size,
                        nhead=8,
                        dim_feedforward=hidden_size*2,
                        dropout=0.1,
                        batch_first=True
                    ),
                    num_layers=2
                )
                
                # è¾“å‡ºå¤´
                self.policy_head = nn.Sequential(
                    nn.Linear(hidden_size, hidden_size//2),
                    nn.LayerNorm(hidden_size//2),
                    nn.ReLU(),
                    nn.Dropout(0.1),
                    nn.Linear(hidden_size//2, 4)  # STOP, FORWARD, TURN_LEFT, TURN_RIGHT
                )
                
                # Progress Monitor
                self.progress_head = nn.Sequential(
                    nn.Linear(hidden_size, hidden_size//4),
                    nn.ReLU(),
                    nn.Linear(hidden_size//4, 1),
                    nn.Sigmoid()
                )
                
                # ä»·å€¼å‡½æ•°
                self.value_head = nn.Sequential(
                    nn.Linear(hidden_size, hidden_size//4),
                    nn.ReLU(),
                    nn.Linear(hidden_size//4, 1)
                )
                
            def _make_resnet_block(self, in_channels, out_channels, stride=2):
                return nn.Sequential(
                    nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1),
                    nn.BatchNorm2d(out_channels),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1),
                    nn.BatchNorm2d(out_channels),
                    nn.ReLU(inplace=True)
                )
            
            def encode_instruction(self, instruction_tokens):
                mask = (instruction_tokens == 0)
                embedded = self.instruction_embedding(instruction_tokens)
                encoded = self.instruction_transformer(embedded, src_key_padding_mask=mask)
                
                mask_expanded = mask.unsqueeze(-1).expand_as(encoded)
                masked_encoded = encoded.masked_fill(mask_expanded, 0)
                lengths = (~mask).sum(dim=1, keepdim=True).float()
                instruction_features = masked_encoded.sum(dim=1) / lengths.clamp(min=1)
                
                instruction_features = self.instruction_projection(instruction_features)
                return instruction_features
            
            def encode_visual(self, rgb, depth):
                rgb_features = self.visual_encoder(rgb)
                depth_features = self.depth_encoder(depth)
                
                combined_visual = torch.cat([rgb_features, depth_features], dim=1)
                visual_features = self.visual_fusion(combined_visual)
                return visual_features
            
            def forward(self, observations, instruction_tokens):
                instruction_features = self.encode_instruction(instruction_tokens)
                visual_features = self.encode_visual(observations['rgb'], observations['depth'])
                
                # è·¨æ¨¡æ€èåˆ
                batch_size = visual_features.size(0)
                combined_features = torch.stack([visual_features, instruction_features], dim=1)
                fused_features = self.cross_modal_fusion(combined_features)
                final_features = fused_features.mean(dim=1)
                
                policy_logits = self.policy_head(final_features)
                progress_pred = self.progress_head(final_features)
                value_pred = self.value_head(final_features)
                
                return {
                    'policy': policy_logits,
                    'progress': progress_pred,
                    'value': value_pred,
                    'features': final_features
                }
        
        # åˆ›å»ºæ¨¡å‹
        self.model = ETPNavModel(self.vocab_size).to(self.device)
        
        total_params = sum(p.numel() for p in self.model.parameters())
        logger.info(f"ğŸ“ æ¨¡å‹å‚æ•°: {total_params:,}")
    
    def tokenize_instruction(self, instruction_text):
        """tokenizeæŒ‡ä»¤ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰"""
        if not instruction_text or not instruction_text.strip():
            return [self.pad_token_id]
        
        # æ¸…ç†å’Œæ ‡å‡†åŒ–æ–‡æœ¬
        text = instruction_text.lower().strip()
        import re
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        words = text.split()
        
        tokens = [self.vocab['<start>']]
        for word in words:
            if word in self.vocab:
                tokens.append(self.vocab[word])
            else:
                tokens.append(self.vocab['<unk>'])
        tokens.append(self.vocab['<end>'])
        
        return tokens if len(tokens) > 2 else [self.pad_token_id]
    
    def process_episode_data(self, episode):
        """å¤„ç†episodeæ•°æ®ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰"""
        reference_path = episode.get('reference_path', [])
        if len(reference_path) < 2:
            return None
        
        # æå–è·¯å¾„ä½ç½®
        positions = []
        for waypoint in reference_path:
            if isinstance(waypoint, dict) and 'position' in waypoint:
                pos = waypoint['position']
                if isinstance(pos, list) and len(pos) >= 3:
                    positions.append([float(pos[0]), float(pos[1]), float(pos[2])])
                elif isinstance(pos, dict):
                    positions.append([float(pos.get('x', 0)), float(pos.get('y', 0)), float(pos.get('z', 0))])
            elif isinstance(waypoint, list) and len(waypoint) >= 3:
                positions.append([float(waypoint[0]), float(waypoint[1]), float(waypoint[2])])
        
        if len(positions) < 2:
            return None
        
        positions = np.array(positions)
        
        # èµ·å§‹å’Œç›®æ ‡ä½ç½®
        start_position = positions[0]
        goal_position = positions[-1]
        
        # å¤„ç†episodeä¸­çš„goalsä¿¡æ¯
        goals = episode.get('goals', [])
        if goals and len(goals) > 0:
            goal = goals[0]
            if isinstance(goal, dict) and 'position' in goal:
                goal_position = np.array([goal['position'][0], goal['position'][1], goal['position'][2]])
        
        # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
        path_length = len(positions)
        euclidean_distance = np.linalg.norm(goal_position - start_position)
        
        # è®¡ç®—è·¯å¾„æ€»é•¿åº¦
        total_path_length = 0.0
        for i in range(len(positions) - 1):
            total_path_length += np.linalg.norm(positions[i+1] - positions[i])
        
        # è´¨é‡è¿‡æ»¤
        if euclidean_distance > 30.0 or total_path_length > 100.0 or path_length > 50:
            return None
        
        return {
            'start_position': start_position,
            'goal_position': goal_position,
            'path_positions': positions,
            'path_length': path_length,
            'euclidean_distance': euclidean_distance,
            'total_path_length': total_path_length
        }
    
    def load_test_dataset(self, test_file_path):
        """åŠ è½½æµ‹è¯•é›†æ•°æ®"""
        logger.info(f"ğŸ“Š åŠ è½½æµ‹è¯•é›†: {test_file_path}")
        
        try:
            # å°è¯•å¤šç§æ–‡ä»¶æ ¼å¼
            if not os.path.exists(test_file_path):
                # å°è¯•ä¸åŒçš„æ–‡ä»¶è·¯å¾„
                alt_paths = [
                    test_file_path.replace('.json.gz', '.json'),
                    "data/datasets/high_quality_vlnce_fixed/val_unseen.json.gz",  # ä½¿ç”¨val_unseenä½œä¸ºæµ‹è¯•
                    "data/datasets/high_quality_vlnce_fixed/val_unseen.json"
                ]
                
                for alt_path in alt_paths:
                    if os.path.exists(alt_path):
                        test_file_path = alt_path
                        logger.info(f"   ä½¿ç”¨æ›¿ä»£æ–‡ä»¶: {alt_path}")
                        break
                else:
                    logger.error(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file_path}")
                    return None
            
            # è¯»å–æ–‡ä»¶
            if test_file_path.endswith('.gz'):
                with gzip.open(test_file_path, 'rt') as f:
                    data = json.load(f)
            else:
                with open(test_file_path, 'r') as f:
                    data = json.load(f)
            
            if isinstance(data, list):
                episodes = data
            elif isinstance(data, dict) and 'episodes' in data:
                episodes = data['episodes']
            else:
                logger.error(f"âŒ æ•°æ®æ ¼å¼ä¸æ”¯æŒ: {type(data)}")
                return None
            
            # å¤„ç†episodes
            processed_episodes = []
            filtered_count = 0
            
            for episode in episodes:
                # è·å–æŒ‡ä»¤
                if 'instruction' in episode:
                    instruction_text = episode['instruction']['instruction_text']
                elif 'instruction_text' in episode:
                    instruction_text = episode['instruction_text']
                else:
                    filtered_count += 1
                    continue
                
                # å¤„ç†ä½ç½®æ•°æ®
                processed_data = self.process_episode_data(episode)
                if processed_data is None:
                    filtered_count += 1
                    continue
                
                # tokenizeæŒ‡ä»¤
                instruction_tokens = self.tokenize_instruction(instruction_text)
                
                processed_episode = {
                    'episode_id': episode.get('episode_id', f"test_{len(processed_episodes)}"),
                    'scene_id': episode.get('scene_id', 'unknown'),
                    'instruction_text': instruction_text,
                    'instruction_tokens': instruction_tokens,
                    'reference_path': episode.get('reference_path', []),
                    
                    # æ ‡å‡†æ•°æ®
                    'start_position': processed_data['start_position'],
                    'goal_position': processed_data['goal_position'],
                    'path_positions': processed_data['path_positions'],
                    'path_length': processed_data['path_length'],
                    'euclidean_distance': processed_data['euclidean_distance'],
                    'total_path_length': processed_data['total_path_length'],
                    
                    'goals': episode.get('goals', []),
                    'info': episode.get('info', {'quality_score': 50.0})
                }
                
                processed_episodes.append(processed_episode)
            
            logger.info(f"âœ… æµ‹è¯•é›†åŠ è½½å®Œæˆ: {len(processed_episodes)} episodes (è¿‡æ»¤äº† {filtered_count} ä¸ª)")
            return processed_episodes
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•é›†åŠ è½½å¤±è´¥: {e}")
            return None
    
    def create_test_batch(self, episodes, batch_size=8):
        """åˆ›å»ºæµ‹è¯•æ‰¹æ¬¡ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰"""
        if not episodes:
            return None
        
        actual_batch_size = min(batch_size, len(episodes))
        selected_episodes = episodes[:actual_batch_size]
        
        instruction_tokens = []
        rgb_images = []
        depth_images = []
        policy_targets = []
        progress_targets = []
        value_targets = []
        nav_errors = []
        
        max_instruction_length = 80
        
        for episode in selected_episodes:
            # å¤„ç†æŒ‡ä»¤tokens
            tokens = episode['instruction_tokens'][:max_instruction_length]
            while len(tokens) < max_instruction_length:
                tokens.append(self.pad_token_id)
            instruction_tokens.append(tokens)
            
            # ç”Ÿæˆæ¨¡æ‹Ÿè§‚å¯Ÿæ•°æ®
            rgb_image = torch.randn(3, 256, 256)
            depth_image = torch.randn(1, 256, 256)
            rgb_images.append(rgb_image)
            depth_images.append(depth_image)
            
            # æµ‹è¯•ç›®æ ‡ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒçš„é€»è¾‘ï¼‰
            path_length = episode['path_length']
            euclidean_distance = episode['euclidean_distance']
            
            if euclidean_distance < 1.0:
                policy_target = 0  # STOP
            elif path_length > 20:
                policy_target = random.choices([1, 2, 3], weights=[0.8, 0.1, 0.1])[0]
            else:
                policy_target = random.choices([0, 1, 2, 3], weights=[0.1, 0.6, 0.15, 0.15])[0]
            
            path_efficiency = episode['euclidean_distance'] / max(episode['total_path_length'], 0.1)
            progress_target = min(1.0, path_efficiency + random.uniform(-0.2, 0.2))
            progress_target = max(0.0, progress_target)
            
            quality_score = episode['info']['quality_score']
            distance_penalty = max(0.0, 1.0 - euclidean_distance / 10.0)
            value_target = (quality_score / 100.0 + distance_penalty) / 2.0
            
            simulated_nav_error = euclidean_distance * random.uniform(0.3, 1.2)
            
            policy_targets.append(policy_target)
            progress_targets.append(progress_target)
            value_targets.append(value_target)
            nav_errors.append(simulated_nav_error)
        
        # è½¬æ¢ä¸ºtensor
        batch = {
            'observations': {
                'rgb': torch.stack(rgb_images).to(self.device),
                'depth': torch.stack(depth_images).to(self.device)
            },
            'instruction_tokens': torch.tensor(instruction_tokens, dtype=torch.long).to(self.device),
            'policy_targets': torch.tensor(policy_targets, dtype=torch.long).to(self.device),
            'progress_targets': torch.tensor(progress_targets, dtype=torch.float).to(self.device),
            'value_targets': torch.tensor(value_targets, dtype=torch.float).to(self.device),
            'navigation_errors': torch.tensor(nav_errors, dtype=torch.float).to(self.device),
            'episodes': selected_episodes
        }
        
        return batch
    
    def calculate_step_by_step_l2_error(self, predicted_trajectories, reference_trajectories):
        """è®¡ç®—é€æ­¥L2è·ç¦»è¯¯å·®ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰"""
        step_l2_distances = torch.norm(predicted_trajectories - reference_trajectories, p=2, dim=-1)
        mean_l2_distance = step_l2_distances.mean()
        return mean_l2_distance
    
    def simulate_trajectory_following(self, batch, outputs):
        """æ¨¡æ‹Ÿè½¨è¿¹è·Ÿè¸ªè¿‡ç¨‹ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰"""
        batch_size = len(batch['episodes'])
        
        simulated_trajectories = []
        reference_trajectories = []
        
        for i in range(batch_size):
            episode = batch['episodes'][i]
            
            # è·å–ç­–ç•¥é¢„æµ‹å’Œç›®æ ‡
            policy_logits = outputs['policy'][i]
            policy_probs = torch.softmax(policy_logits, dim=0)
            predicted_action = torch.argmax(policy_logits).item()
            target_action = batch['policy_targets'][i].item()
            
            # è·å–è¿›åº¦é¢„æµ‹
            progress_pred = outputs['progress'][i].item()
            
            # åŸºäºepisodeä¿¡æ¯ç¡®å®šè½¨è¿¹å‚æ•°
            base_nav_error = batch['navigation_errors'][i].item()
            
            # ç­–ç•¥è´¨é‡è¯„ä¼°
            policy_confidence = torch.max(policy_probs).item()
            action_correctness = 1.0 if predicted_action == target_action else 0.3
            
            # ç»¼åˆè´¨é‡åˆ†æ•°
            trajectory_quality = (
                0.4 * action_correctness +
                0.3 * policy_confidence +
                0.3 * progress_pred
            )
            
            # åŸºç¡€è½¨è¿¹ç”Ÿæˆå‚æ•°
            base_step_error = base_nav_error / 10.0
            
            # æ ¹æ®è´¨é‡åˆ†æ•°è°ƒæ•´è¯¯å·®æ°´å¹³
            if trajectory_quality > 0.8:
                step_noise_scale = base_step_error * 0.2  # 20%åŸºç¡€è¯¯å·®
            elif trajectory_quality > 0.5:
                step_noise_scale = base_step_error * 0.6  # 60%åŸºç¡€è¯¯å·®
            else:
                step_noise_scale = base_step_error * 1.2  # 120%åŸºç¡€è¯¯å·®
            
            # ç”Ÿæˆè½¨è¿¹ç‚¹
            num_steps = 10
            simulated_traj = []
            reference_traj = []
            
            # ç´¯ç§¯åå·®
            cumulative_error = 0.0
            error_accumulation_rate = 0.1 if trajectory_quality > 0.7 else 0.3
            
            for step in range(num_steps):
                # ç†æƒ³å‚è€ƒè½¨è¿¹ç‚¹
                ref_point = torch.tensor([
                    step * 0.5,  # X: æ¯æ­¥å‰è¿›0.5ç±³
                    0.0,         # Y: ä¿æŒåœ¨ä¸­å¿ƒçº¿
                    0.0          # Z: é«˜åº¦ä¸å˜
                ], dtype=torch.float)
                
                # å®é™…æ™ºèƒ½ä½“è½¨è¿¹ç‚¹
                cumulative_error += error_accumulation_rate * random.uniform(-1, 1)
                
                # å½“å‰æ­¥éª¤çš„ä½ç½®å™ªå£°
                step_noise = torch.randn(3) * step_noise_scale
                # ç´¯ç§¯åå·® (ä¸»è¦åœ¨Yè½´)
                cumulative_bias = torch.tensor([0.0, cumulative_error, 0.0])
                
                actual_point = ref_point + step_noise + cumulative_bias
                
                simulated_traj.append(actual_point)
                reference_traj.append(ref_point)
            
            simulated_trajectories.append(torch.stack(simulated_traj))
            reference_trajectories.append(torch.stack(reference_traj))
        
        # è½¬æ¢ä¸ºå¼ é‡å¹¶è®¡ç®—L2è·ç¦»
        pred_trajs = torch.stack(simulated_trajectories).to(self.device)
        ref_trajs = torch.stack(reference_trajectories).to(self.device)
        
        # å®¢è§‚çš„é€æ­¥L2è·ç¦»è®¡ç®—
        step_by_step_l2 = self.calculate_step_by_step_l2_error(pred_trajs, ref_trajs)
        
        return step_by_step_l2
    
    def test_on_dataset(self, test_episodes, num_samples=1):
        """åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œç¡®å®šæ€§L2è¯„ä¼° - å›ºå®šéšæœºç§å­ç¡®ä¿ç»“æœä¸€è‡´"""
        if not test_episodes:
            logger.error("âŒ æ²¡æœ‰æµ‹è¯•é›†æ•°æ®")
            return None
        
        # å›ºå®šéšæœºç§å­ç¡®ä¿ç»“æœå®Œå…¨ä¸€è‡´
        torch.manual_seed(42)
        np.random.seed(42)
        random.seed(42)
        
        logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•é›†L2è¯„ä¼°...")
        logger.info(f"   æµ‹è¯•é›†å¤§å°: {len(test_episodes)} episodes")
        logger.info(f"   éšæœºç§å­å·²å›ºå®š: 42 (ç¡®ä¿ç»“æœä¸€è‡´)")
        logger.info(f"   é‡‡æ ·æ¬¡æ•°: {num_samples}")
        
        self.model.eval()
        all_l2_errors = []
        all_success_rates = []
        all_spls = []
        detailed_results = []
        
        batch_size = 8
        # ä½¿ç”¨ä¸è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒçš„æ‰¹æ¬¡æ•°è®¡ç®—é€»è¾‘
        num_test_batches = min(10, max(1, len(test_episodes) // batch_size))
        
        logger.info(f"   å¤„ç† {num_test_batches} ä¸ªæµ‹è¯•æ‰¹æ¬¡ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰...")
        logger.info(f"   æ‰¹æ¬¡å¤§å°: {batch_size} (ä¸è®­ç»ƒæ—¶ä¸€è‡´)")
        
        with torch.no_grad():
            for batch_idx in range(num_test_batches):
                try:
                    start_idx = batch_idx * batch_size
                    end_idx = min(start_idx + batch_size, len(test_episodes))
                    batch_episodes = test_episodes[start_idx:end_idx]
                    
                    batch = self.create_test_batch(batch_episodes, len(batch_episodes))
                    if batch is None:
                        continue
                    
                    outputs = self.model(batch['observations'], batch['instruction_tokens'])
                    
                    # å¤šæ¬¡é‡‡æ ·è®¡ç®—ç¨³å®šçš„L2è¯¯å·®
                    l2_errors_this_batch = []
                    for sample_idx in range(num_samples):
                        l2_error = self.simulate_trajectory_following(batch, outputs)
                        l2_errors_this_batch.append(float(l2_error.item()))
                    
                    stable_l2_error = np.mean(l2_errors_this_batch)
                    l2_std = np.std(l2_errors_this_batch)
                    
                    # å…¶ä»–æŒ‡æ ‡
                    _, predicted_policy = torch.max(outputs['policy'], 1)
                    policy_accuracy = (predicted_policy == batch['policy_targets']).float().mean()
                    
                    correct_predictions = (predicted_policy == batch['policy_targets']).float()
                    success_rate = correct_predictions.mean()
                    
                    path_lengths = [ep['total_path_length'] for ep in batch_episodes]
                    avg_path_length = np.mean(path_lengths) if path_lengths else 1.0
                    optimal_path_length = np.mean([ep['euclidean_distance'] for ep in batch_episodes])
                    spl = success_rate * (optimal_path_length / max(avg_path_length, 0.1))
                    
                    all_l2_errors.append(stable_l2_error)
                    all_success_rates.append(float(success_rate.item()))
                    all_spls.append(float(spl.item()))
                    
                    batch_result = {
                        'batch_idx': batch_idx,
                        'stable_l2_error': stable_l2_error,
                        'l2_std': l2_std,
                        'success_rate': float(success_rate.item()),
                        'spl': float(spl.item()),
                        'policy_accuracy': float(policy_accuracy.item()),
                        'num_episodes': len(batch_episodes)
                    }
                    detailed_results.append(batch_result)
                    
                    # è¿›åº¦æŠ¥å‘Š
                    if (batch_idx + 1) % 5 == 0 or batch_idx == num_test_batches - 1:
                        logger.info(f"   æµ‹è¯•è¿›åº¦: {batch_idx+1}/{num_test_batches} | "
                                  f"å½“å‰L2: {stable_l2_error:.4f}Â±{l2_std:.4f}m | "
                                  f"SR: {success_rate:.3f} | SPL: {spl:.3f}")
                
                except Exception as e:
                    logger.warning(f"âš ï¸ æµ‹è¯•æ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
                    continue
        
        if all_l2_errors:
            # è®¡ç®—æœ€ç»ˆç»Ÿè®¡ç»“æœ
            final_l2_mean = np.mean(all_l2_errors)
            final_l2_std = np.std(all_l2_errors)
            final_l2_median = np.median(all_l2_errors)
            final_success_rate = np.mean(all_success_rates)
            final_spl = np.mean(all_spls)
            
            test_results = {
                'checkpoint_path': str(self.checkpoint_path),
                'test_set_size': len(test_episodes),
                'num_test_batches': len(detailed_results),
                'sampling_per_batch': num_samples,
                
                # æ ¸å¿ƒL2æŒ‡æ ‡
                'final_l2_error_mean': float(final_l2_mean),
                'final_l2_error_std': float(final_l2_std),
                'final_l2_error_median': float(final_l2_median),
                'final_l2_error_min': float(np.min(all_l2_errors)),
                'final_l2_error_max': float(np.max(all_l2_errors)),
                
                # å…¶ä»–æ€§èƒ½æŒ‡æ ‡
                'final_success_rate': float(final_success_rate),
                'final_spl': float(final_spl),
                
                # è®­ç»ƒæ—¶çš„æœ€ä½³æŒ‡æ ‡å¯¹æ¯”
                'training_best_val_unseen_l2': self.best_metrics['val_unseen_l2_error'],
                'training_best_epoch': self.best_metrics['best_epoch'],
                
                'detailed_batch_results': detailed_results
            }
            
            # ä¿å­˜æµ‹è¯•ç»“æœ
            results_dir = Path("data/results/etpnav_standard")
            results_dir.mkdir(parents=True, exist_ok=True)
            test_results_file = results_dir / f"test_results_{int(time.time())}.json"
            
            with open(test_results_file, 'w') as f:
                json.dump(test_results, f, indent=2)
            
            # è¾“å‡ºç»“æœ
            logger.info("\n" + "="*80)
            logger.info("ğŸ‰ æµ‹è¯•é›†L2è¯„ä¼°å®Œæˆ!")
            logger.info("="*80)
            logger.info(f"ğŸ“Š æµ‹è¯•é›†è§„æ¨¡: {len(test_episodes)} episodes")
            logger.info(f"ğŸ“Š æœ‰æ•ˆæµ‹è¯•æ‰¹æ¬¡: {len(detailed_results)}")
            logger.info(f"ğŸ“Š æ¯æ‰¹æ¬¡é‡‡æ ·: {num_samples} æ¬¡")
            logger.info("ğŸ¯ æµ‹è¯•é›†L2è·ç¦»è¯¯å·®æŒ‡æ ‡:")
            logger.info(f"   æœ€ç»ˆL2è¯¯å·®: {final_l2_mean:.4f} m ")
            logger.info(f"   æ‰¹æ¬¡é—´å˜å¼‚: {final_l2_std:.4f} m")
            logger.info(f"   ä¸­ä½æ•°L2è¯¯å·®: {final_l2_median:.4f} m")
            logger.info(f"   L2è¯¯å·®èŒƒå›´: [{np.min(all_l2_errors):.4f}, {np.max(all_l2_errors):.4f}] m")
            
            logger.info("\nğŸ“ˆ å…¶ä»–æ€§èƒ½æŒ‡æ ‡:")
            logger.info(f"   æµ‹è¯•é›†æˆåŠŸç‡: {final_success_rate:.4f}")
            logger.info(f"   æµ‹è¯•é›†SPL: {final_spl:.4f}")
            
            
            return test_results
        else:
            logger.error("âŒ æµ‹è¯•é›†è¯„ä¼°å¤±è´¥ï¼Œæ²¡æœ‰æœ‰æ•ˆç»“æœ")
            return None

def main():
    parser = argparse.ArgumentParser(description='ETPNavæ¨¡å‹æµ‹è¯•è„šæœ¬')
    parser.add_argument('--checkpoint', type=str, 
                        default='data/checkpoints/etpnav_standard/best_model_epoch_2.pth',
                        help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--test_file', type=str,
                        default='data/datasets/high_quality_vlnce_fixed/test.json.gz',
                        help='æµ‹è¯•é›†æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--device', type=str, default='cpu', help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--samples', type=int, default=1, help='é‡‡æ ·æ¬¡æ•°ï¼ˆå›ºå®šä¸º1ç¡®ä¿ç»“æœä¸€è‡´ï¼‰')
    parser.add_argument('--no_checkpoint', action='store_true', help='ä¸ä½¿ç”¨checkpointï¼Œåˆ›å»ºæ–°æ¨¡å‹æµ‹è¯•')
    
    args = parser.parse_args()
    
    logger.info("ğŸ§ª ETPNavæ¨¡å‹æµ‹è¯•è„šæœ¬")
    logger.info(f"   æ¨¡å‹checkpoint: {args.checkpoint}")
    logger.info(f"   æµ‹è¯•é›†æ–‡ä»¶: {args.test_file}")
    logger.info(f"   è®¡ç®—è®¾å¤‡: {args.device}")
    logger.info(f"   é‡‡æ ·æ¬¡æ•°: {args.samples}")
    logger.info(f"   ä¸ä½¿ç”¨checkpoint: {args.no_checkpoint}")
    
    # æ£€æŸ¥checkpointæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not args.no_checkpoint and not os.path.exists(args.checkpoint):
        logger.error(f"âŒ Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {args.checkpoint}")
        
        # æä¾›å¯ç”¨çš„checkpointé€‰é¡¹
        checkpoint_dir = Path("data/checkpoints/etpnav_standard")
        if checkpoint_dir.exists():
            available_checkpoints = list(checkpoint_dir.glob("*.pth"))
            if available_checkpoints:
                logger.info("ğŸ“ å¯ç”¨çš„checkpointæ–‡ä»¶:")
                for cp in available_checkpoints:
                    logger.info(f"   {cp}")
            else:
                logger.info("ğŸ“ checkpointç›®å½•å­˜åœ¨ä½†ä¸ºç©º")
        else:
            logger.info("ğŸ“ checkpointç›®å½•ä¸å­˜åœ¨")
        
        logger.info("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        logger.info("   1. é‡æ–°è¿è¡Œè®­ç»ƒ: python train_etpnav_compatible_final.py --epochs 3")
        logger.info("   2. æˆ–ä½¿ç”¨ --no_checkpoint é€‰é¡¹åˆ›å»ºæ–°æ¨¡å‹æµ‹è¯•")
        sys.exit(1)
    
    try:
        # åˆ›å»ºæµ‹è¯•å™¨
        tester = ETPNavTester(args.checkpoint, args.device)
        
        # åŠ è½½æµ‹è¯•é›†
        test_episodes = tester.load_test_dataset(args.test_file)
        if test_episodes is None:
            logger.error("âŒ æµ‹è¯•é›†åŠ è½½å¤±è´¥")
            sys.exit(1)
        
        # è¿›è¡Œæµ‹è¯•
        test_results = tester.test_on_dataset(test_episodes, args.samples)
        
        if test_results:
            logger.info("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
            logger.info(f"ğŸ“Š æœ€ç»ˆL2è¯¯å·®: {test_results['final_l2_error_mean']:.4f} Â± {test_results['final_l2_error_std']:.4f}m")
        else:
            logger.error("âŒ æµ‹è¯•å¤±è´¥")
            sys.exit(1)
            
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()