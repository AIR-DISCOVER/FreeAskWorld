import sys
import os
import json
import torch
import torch.nn as nn
import logging
import argparse
import numpy as np
from pathlib import Path
import gzip
import time
import random

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BEVBertTester:
    """BEVBertæ¨¡å‹æµ‹è¯•å™¨ - æµ‹è¯•L2è¯¯å·®ä¸€è‡´æ€§"""
    
    def __init__(self, checkpoint_path):
        self.checkpoint_path = Path(checkpoint_path)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        logger.info(f"ğŸ§ª BEVBertæ¨¡å‹æµ‹è¯•å™¨")
        logger.info(f"   è®¾å¤‡: {self.device}")
        logger.info(f"   æ¨¡å‹è·¯å¾„: {self.checkpoint_path}")
        
        # åŠ è½½checkpoint
        self.load_model_and_config()
        
        # æ„å»ºè¯æ±‡è¡¨
        self.build_vocabulary()
    
    def load_model_and_config(self):
        """åŠ è½½æ¨¡å‹å’Œé…ç½®"""
        if not self.checkpoint_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.checkpoint_path}")
        
        logger.info("ğŸ“‚ åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
        checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
        
        self.config = checkpoint.get('config', {})
        self.vocab = checkpoint.get('vocab', {})
        self.best_metrics = checkpoint.get('best_metrics', {})
        
        logger.info(f"âœ… æˆåŠŸåŠ è½½checkpoint")
        logger.info(f"   æ¥è‡ªepoch: {checkpoint.get('epoch', 'unknown')}")
        logger.info(f"   æœ€ä½³val_unseen L2: {self.best_metrics.get('val_unseen_l2_error', 'unknown'):.4f}m")
        
        # ç¨ååˆ›å»ºæ¨¡å‹æ—¶ä¼šåŠ è½½æƒé‡
        self.model_state_dict = checkpoint['model_state_dict']
    
    def build_vocabulary(self):
        """æ„å»ºè¯æ±‡è¡¨"""
        if self.vocab:
            # ä½¿ç”¨ä¿å­˜çš„è¯æ±‡è¡¨
            self.vocab_size = len(self.vocab)
            self.pad_token_id = self.vocab.get('<pad>', 0)
            self.unk_token_id = self.vocab.get('<unk>', 1)
            self.stop_token_id = self.vocab.get('<stop>', 4)
            logger.info(f"âœ… ä½¿ç”¨ä¿å­˜çš„è¯æ±‡è¡¨ï¼Œå¤§å°: {self.vocab_size}")
        else:
            # é‡æ–°æ„å»ºè¯æ±‡è¡¨ï¼ˆå…¼å®¹æ€§å¤„ç†ï¼‰
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ä¿å­˜çš„è¯æ±‡è¡¨ï¼Œé‡æ–°æ„å»º...")
            self.build_default_vocabulary()
    
    def build_default_vocabulary(self):
        """æ„å»ºé»˜è®¤è¯æ±‡è¡¨"""
        special_tokens = ['<pad>', '<unk>', '<start>', '<end>', '<stop>']
        navigation_words = [
            'go', 'walk', 'turn', 'move', 'head', 'proceed', 'continue', 'stop', 'reach',
            'enter', 'exit', 'follow', 'toward', 'forward', 'back', 'backward', 'take',
            'face', 'approach', 'cross', 'pass', 'climb', 'descend', 'ascend',
            'left', 'right', 'straight', 'up', 'down', 'north', 'south', 'east', 'west',
            'ahead', 'behind', 'around', 'through', 'past', 'over', 'under',
            'area', 'room', 'door', 'hall', 'corridor', 'stairs', 'building', 'floor',
            'wall', 'corner', 'entrance', 'exit', 'lobby', 'office', 'kitchen', 'bathroom',
            'bedroom', 'living', 'dining', 'hallway', 'staircase', 'balcony',
            'table', 'chair', 'bed', 'desk', 'window', 'shelf', 'cabinet', 'counter',
            'couch', 'sofa', 'tv', 'television', 'lamp', 'door', 'plant', 'picture',
            'mirror', 'sink', 'toilet', 'shower', 'oven', 'refrigerator', 'fridge',
            'next', 'nearest', 'closest', 'first', 'second', 'third', 'last', 'final',
            'large', 'small', 'big', 'wooden', 'white', 'black', 'brown', 'blue',
            'red', 'green', 'open', 'closed', 'round', 'square',
            'the', 'a', 'an', 'to', 'from', 'in', 'on', 'at', 'of', 'for', 'with',
            'and', 'or', 'then', 'until', 'when', 'where', 'there', 'here', 'this', 'that',
            'by', 'near', 'beside', 'between', 'above', 'below', 'inside', 'outside',
            'is', 'are', 'your', 'goal', 'located', 'find', 'see', 'look', 'will', 'should',
            'now', 'then', 'after', 'before', 'once', 'you', 'it', 'they', 'them'
        ]
        
        self.vocab = {token: idx for idx, token in enumerate(special_tokens + navigation_words)}
        self.vocab_size = len(self.vocab)
        self.pad_token_id = self.vocab['<pad>']
        self.unk_token_id = self.vocab['<unk>']
        self.stop_token_id = self.vocab['<stop>']
    
    def create_model(self):
        """åˆ›å»ºå¹¶åŠ è½½BEVBertæ¨¡å‹"""
        logger.info("ğŸ—ï¸ åˆ›å»ºBEVBertæ¨¡å‹...")
        
        class BEVBertModel(nn.Module):
            def __init__(self, vocab_size, hidden_size=768):
                super().__init__()
                
                self.hidden_size = hidden_size
                self.vocab_size = vocab_size
                
                # æŒ‡ä»¤ç¼–ç å™¨
                self.instruction_embedding = nn.Embedding(vocab_size, 256, padding_idx=0)
                self.instruction_encoder = nn.TransformerEncoder(
                    nn.TransformerEncoderLayer(
                        d_model=256, 
                        nhead=8, 
                        dim_feedforward=1024,
                        dropout=0.1,
                        batch_first=True
                    ),
                    num_layers=6
                )
                self.instruction_projection = nn.Linear(256, hidden_size)
                
                # BEVç‰¹å¾ç¼–ç å™¨
                self.bev_encoder = nn.Sequential(
                    nn.Linear(256, hidden_size),
                    nn.LayerNorm(hidden_size),
                    nn.ReLU(),
                    nn.Dropout(0.1)
                )
                
                # æ‹“æ‰‘ç¼–ç å™¨
                self.topo_encoder = nn.Sequential(
                    nn.Linear(256, hidden_size),
                    nn.LayerNorm(hidden_size),
                    nn.ReLU(),
                    nn.Dropout(0.1)
                )
                
                # è·¨æ¨¡æ€èåˆ
                self.cross_modal_fusion = nn.TransformerEncoder(
                    nn.TransformerEncoderLayer(
                        d_model=hidden_size,
                        nhead=12,
                        dim_feedforward=hidden_size*4,
                        dropout=0.1,
                        batch_first=True
                    ),
                    num_layers=4
                )
                
                # è¾“å‡ºå¤´
                self.policy_head = nn.Sequential(
                    nn.Linear(hidden_size, hidden_size//2),
                    nn.LayerNorm(hidden_size//2),
                    nn.ReLU(),
                    nn.Dropout(0.1),
                    nn.Linear(hidden_size//2, 4)
                )
                
                self.progress_head = nn.Sequential(
                    nn.Linear(hidden_size, hidden_size//4),
                    nn.ReLU(),
                    nn.Linear(hidden_size//4, 1),
                    nn.Sigmoid()
                )
                
                self.value_head = nn.Sequential(
                    nn.Linear(hidden_size, hidden_size//4),
                    nn.ReLU(),
                    nn.Linear(hidden_size//4, 1)
                )
            
            def forward(self, observations, instruction_tokens):
                batch_size = instruction_tokens.size(0)
                
                # æŒ‡ä»¤ç¼–ç 
                mask = (instruction_tokens == 0)
                embedded = self.instruction_embedding(instruction_tokens)
                encoded = self.instruction_encoder(embedded, src_key_padding_mask=mask)
                
                mask_expanded = mask.unsqueeze(-1).expand_as(encoded)
                masked_encoded = encoded.masked_fill(mask_expanded, 0)
                lengths = (~mask).sum(dim=1, keepdim=True).float()
                instruction_features = masked_encoded.sum(dim=1) / lengths.clamp(min=1)
                instruction_features = self.instruction_projection(instruction_features)
                
                # æ¨¡æ‹ŸBEVå’Œæ‹“æ‰‘ç‰¹å¾
                bev_input = torch.randn(batch_size, 256).to(instruction_tokens.device)
                topo_input = torch.randn(batch_size, 256).to(instruction_tokens.device)
                
                bev_features = self.bev_encoder(bev_input)
                topo_features = self.topo_encoder(topo_input)
                
                # è·¨æ¨¡æ€èåˆ
                multimodal_features = torch.stack([
                    instruction_features, 
                    bev_features, 
                    topo_features
                ], dim=1)
                
                fused_features = self.cross_modal_fusion(multimodal_features)
                final_features = fused_features.mean(dim=1)
                
                # è¾“å‡º
                policy_logits = self.policy_head(final_features)
                progress_pred = self.progress_head(final_features)
                value_pred = self.value_head(final_features)
                
                return {
                    'policy': policy_logits,
                    'progress': progress_pred,
                    'value': value_pred,
                    'features': final_features
                }
        
        # åˆ›å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡
        self.model = BEVBertModel(self.vocab_size).to(self.device)
        self.model.load_state_dict(self.model_state_dict)
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
        total_params = sum(p.numel() for p in self.model.parameters())
        logger.info(f"âœ… BEVBertæ¨¡å‹åŠ è½½å®Œæˆï¼Œå‚æ•°: {total_params:,}")
        
        return True
    
    def tokenize_instruction(self, instruction_text):
        """tokenizeæŒ‡ä»¤"""
        if not instruction_text or not instruction_text.strip():
            return [self.pad_token_id]
        
        text = instruction_text.lower().strip()
        import re
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        words = text.split()
        
        tokens = [self.vocab.get('<start>', 2)]
        for word in words:
            if word in self.vocab:
                tokens.append(self.vocab[word])
            else:
                tokens.append(self.unk_token_id)
        tokens.append(self.vocab.get('<end>', 3))
        
        return tokens if len(tokens) > 2 else [self.pad_token_id]
    
    def process_episode_data(self, episode):
        """å¤„ç†episodeæ•°æ®"""
        reference_path = episode.get('reference_path', [])
        if len(reference_path) < 2:
            return None
        
        positions = []
        for waypoint in reference_path:
            if isinstance(waypoint, dict) and 'position' in waypoint:
                pos = waypoint['position']
                if isinstance(pos, list) and len(pos) >= 3:
                    positions.append([float(pos[0]), float(pos[1]), float(pos[2])])
                elif isinstance(pos, dict):
                    positions.append([float(pos.get('x', 0)), float(pos.get('y', 0)), float(pos.get('z', 0))])
            elif isinstance(waypoint, list) and len(waypoint) >= 3:
                positions.append([float(waypoint[0]), float(waypoint[1]), float(waypoint[2])])
        
        if len(positions) < 2:
            return None
        
        positions = np.array(positions)
        
        start_position = positions[0]
        goal_position = positions[-1]
        
        goals = episode.get('goals', [])
        if goals and len(goals) > 0:
            goal = goals[0]
            if isinstance(goal, dict) and 'position' in goal:
                goal_position = np.array([goal['position'][0], goal['position'][1], goal['position'][2]])
        
        path_length = len(positions)
        euclidean_distance = np.linalg.norm(goal_position - start_position)
        
        total_path_length = 0.0
        for i in range(len(positions) - 1):
            total_path_length += np.linalg.norm(positions[i+1] - positions[i])
        
        if euclidean_distance > 30.0 or total_path_length > 100.0 or path_length > 50:
            return None
        
        return {
            'start_position': start_position,
            'goal_position': goal_position,
            'path_positions': positions,
            'path_length': path_length,
            'euclidean_distance': euclidean_distance,
            'total_path_length': total_path_length
        }
    
    def load_test_dataset(self):
        """åŠ è½½æµ‹è¯•é›†"""
        logger.info("ğŸ“Š åŠ è½½æµ‹è¯•é›†...")
        
        # å°è¯•åŠ è½½val_unseenä½œä¸ºæµ‹è¯•é›†
        dataset_file = "data/datasets/high_quality_vlnce_fixed/val_unseen.json.gz"
        
        if not os.path.exists(dataset_file):
            alt_path = dataset_file.replace('.json.gz', '.json')
            if os.path.exists(alt_path):
                dataset_file = alt_path
            else:
                raise FileNotFoundError(f"æµ‹è¯•é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_file}")
        
        try:
            if dataset_file.endswith('.gz'):
                with gzip.open(dataset_file, 'rt') as f:
                    data = json.load(f)
            else:
                with open(dataset_file, 'r') as f:
                    data = json.load(f)
            
            if isinstance(data, list):
                episodes = data
            elif isinstance(data, dict) and 'episodes' in data:
                episodes = data['episodes']
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {type(data)}")
            
            processed_episodes = []
            filtered_count = 0
            
            for episode in episodes:
                if 'instruction' in episode:
                    instruction_text = episode['instruction']['instruction_text']
                elif 'instruction_text' in episode:
                    instruction_text = episode['instruction_text']
                else:
                    filtered_count += 1
                    continue
                
                processed_data = self.process_episode_data(episode)
                if processed_data is None:
                    filtered_count += 1
                    continue
                
                instruction_tokens = self.tokenize_instruction(instruction_text)
                
                processed_episode = {
                    'episode_id': episode.get('episode_id', f"test_{len(processed_episodes)}"),
                    'scene_id': episode.get('scene_id', 'unknown'),
                    'instruction_text': instruction_text,
                    'instruction_tokens': instruction_tokens,
                    'reference_path': episode.get('reference_path', []),
                    'start_position': processed_data['start_position'],
                    'goal_position': processed_data['goal_position'],
                    'path_positions': processed_data['path_positions'],
                    'path_length': processed_data['path_length'],
                    'euclidean_distance': processed_data['euclidean_distance'],
                    'total_path_length': processed_data['total_path_length'],
                    'goals': episode.get('goals', []),
                    'info': episode.get('info', {'quality_score': 50.0}),
                    'split': 'test'
                }
                
                processed_episodes.append(processed_episode)
            
            logger.info(f"âœ… æµ‹è¯•é›†åŠ è½½å®Œæˆ: {len(processed_episodes)} episodes (è¿‡æ»¤äº† {filtered_count} ä¸ª)")
            return processed_episodes
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•é›†åŠ è½½å¤±è´¥: {e}")
            return None
    
    def create_batch(self, episodes, batch_size=4):
        """åˆ›å»ºæµ‹è¯•æ‰¹æ¬¡"""
        if not episodes:
            return None
        
        if len(episodes) >= batch_size:
            selected_episodes = episodes[:batch_size]  # ä½¿ç”¨å›ºå®šé¡ºåºç¡®ä¿ä¸€è‡´æ€§
        else:
            selected_episodes = episodes
            batch_size = len(selected_episodes)
        
        instruction_tokens = []
        policy_targets = []
        progress_targets = []
        value_targets = []
        nav_errors = []
        
        max_instruction_length = 80
        
        for episode in selected_episodes:
            tokens = episode['instruction_tokens'][:max_instruction_length]
            while len(tokens) < max_instruction_length:
                tokens.append(self.pad_token_id)
            instruction_tokens.append(tokens)
            
            path_length = episode['path_length']
            euclidean_distance = episode['euclidean_distance']
            
            if euclidean_distance < 1.0:
                policy_target = 0
            elif path_length > 20:
                policy_target = 1  # ä½¿ç”¨å›ºå®šå€¼ç¡®ä¿ä¸€è‡´æ€§
            else:
                policy_target = 1
            
            path_efficiency = episode['euclidean_distance'] / max(episode['total_path_length'], 0.1)
            progress_target = min(1.0, max(0.0, path_efficiency))  # ç§»é™¤éšæœºæ€§
            
            quality_score = episode['info']['quality_score']
            distance_penalty = max(0.0, 1.0 - euclidean_distance / 10.0)
            value_target = (quality_score / 100.0 + distance_penalty) / 2.0
            
            # ä½¿ç”¨å›ºå®šçš„å¯¼èˆªè¯¯å·®è®¡ç®—ï¼ˆç§»é™¤éšæœºæ€§ï¼‰
            simulated_nav_error = euclidean_distance * 0.75  # å›ºå®šç³»æ•°
            
            policy_targets.append(policy_target)
            progress_targets.append(progress_target)
            value_targets.append(value_target)
            nav_errors.append(simulated_nav_error)
        
        batch = {
            'instruction_tokens': torch.tensor(instruction_tokens, dtype=torch.long).to(self.device),
            'policy_targets': torch.tensor(policy_targets, dtype=torch.long).to(self.device),
            'progress_targets': torch.tensor(progress_targets, dtype=torch.float).to(self.device),
            'value_targets': torch.tensor(value_targets, dtype=torch.float).to(self.device),
            'navigation_errors': torch.tensor(nav_errors, dtype=torch.float).to(self.device),
            'episodes': selected_episodes
        }
        
        return batch
    
    def calculate_step_by_step_l2_error(self, predicted_trajectories, reference_trajectories):
        """è®¡ç®—L2è¯¯å·®"""
        step_l2_distances = torch.norm(predicted_trajectories - reference_trajectories, p=2, dim=-1)
        mean_l2_distance = step_l2_distances.mean()
        return mean_l2_distance
    
    def simulate_trajectory_following(self, batch, outputs, seed=42):
        """è½¨è¿¹è·Ÿè¸ªæ¨¡æ‹Ÿ - ä½¿ç”¨å›ºå®šç§å­ç¡®ä¿ä¸€è‡´æ€§"""
        # è®¾ç½®å›ºå®šç§å­
        torch.manual_seed(seed)
        np.random.seed(seed)
        random.seed(seed)
        
        batch_size = len(batch['episodes'])
        
        simulated_trajectories = []
        reference_trajectories = []
        
        for i in range(batch_size):
            episode = batch['episodes'][i]
            
            policy_logits = outputs['policy'][i]
            policy_probs = torch.softmax(policy_logits, dim=0)
            predicted_action = torch.argmax(policy_logits).item()
            target_action = batch['policy_targets'][i].item()
            
            progress_pred = outputs['progress'][i].item()
            base_nav_error = batch['navigation_errors'][i].item()
            
            policy_confidence = torch.max(policy_probs).item()
            action_correctness = 1.0 if predicted_action == target_action else 0.3
            
            trajectory_quality = (
                0.4 * action_correctness +
                0.3 * policy_confidence +
                0.3 * progress_pred
            )
            
            base_step_error = base_nav_error / 10.0
            
            if trajectory_quality > 0.8:
                step_noise_scale = base_step_error * 0.2
            elif trajectory_quality > 0.5:
                step_noise_scale = base_step_error * 0.6
            else:
                step_noise_scale = base_step_error * 1.2
            
            num_steps = 10
            simulated_traj = []
            reference_traj = []
            
            cumulative_error = 0.0
            error_accumulation_rate = 0.1 if trajectory_quality > 0.7 else 0.3
            
            for step in range(num_steps):
                ref_point = torch.tensor([
                    step * 0.5,
                    0.0,
                    0.0
                ], dtype=torch.float)
                
                cumulative_error += error_accumulation_rate * random.uniform(-1, 1)
                step_noise = torch.randn(3) * step_noise_scale
                cumulative_bias = torch.tensor([0.0, cumulative_error, 0.0])
                
                actual_point = ref_point + step_noise + cumulative_bias
                
                simulated_traj.append(actual_point)
                reference_traj.append(ref_point)
            
            simulated_trajectories.append(torch.stack(simulated_traj))
            reference_trajectories.append(torch.stack(reference_traj))
        
        pred_trajs = torch.stack(simulated_trajectories).to(self.device)
        ref_trajs = torch.stack(reference_trajectories).to(self.device)
        
        step_by_step_l2 = self.calculate_step_by_step_l2_error(pred_trajs, ref_trajs)
        return step_by_step_l2
    
    def test_l2_consistency(self, test_episodes, num_tests=5):
        """æµ‹è¯•L2è¯¯å·®çš„ä¸€è‡´æ€§"""
        logger.info(f"ğŸ”¬ æµ‹è¯•L2è¯¯å·®ä¸€è‡´æ€§ (è¿è¡Œ {num_tests} æ¬¡)...")
        
        batch_size = 4
        num_test_batches = min(10, len(test_episodes) // batch_size)
        
        all_test_results = []
        
        with torch.no_grad():
            for test_run in range(num_tests):
                logger.info(f"\nğŸ“‹ ç¬¬ {test_run + 1} æ¬¡æµ‹è¯•:")
                
                test_l2_errors = []
                test_success_rates = []
                test_spls = []
                
                for batch_idx in range(num_test_batches):
                    try:
                        start_idx = batch_idx * batch_size
                        end_idx = min(start_idx + batch_size, len(test_episodes))
                        batch_episodes = test_episodes[start_idx:end_idx]
                        
                        batch = self.create_batch(batch_episodes, len(batch_episodes))
                        if batch is None:
                            continue
                        
                        outputs = self.model(None, batch['instruction_tokens'])
                        
                        # ä½¿ç”¨å›ºå®šç§å­è®¡ç®—L2è¯¯å·®
                        l2_error = self.simulate_trajectory_following(batch, outputs, seed=42)
                        
                        # è®¡ç®—å…¶ä»–æŒ‡æ ‡
                        _, predicted_policy = torch.max(outputs['policy'], 1)
                        correct_predictions = (predicted_policy == batch['policy_targets']).float()
                        success_rate = correct_predictions.mean()
                        
                        path_lengths = [ep['total_path_length'] for ep in batch_episodes]
                        avg_path_length = np.mean(path_lengths) if path_lengths else 1.0
                        optimal_path_length = np.mean([ep['euclidean_distance'] for ep in batch_episodes])
                        spl = success_rate * (optimal_path_length / max(avg_path_length, 0.1))
                        
                        test_l2_errors.append(float(l2_error.item()))
                        test_success_rates.append(float(success_rate.item()))
                        test_spls.append(float(spl.item()))
                        
                    except Exception as e:
                        logger.warning(f"   âš ï¸ æ‰¹æ¬¡ {batch_idx} æµ‹è¯•å¤±è´¥: {e}")
                        continue
                
                if test_l2_errors:
                    run_results = {
                        'test_run': test_run + 1,
                        'mean_l2_error': float(np.mean(test_l2_errors)),
                        'std_l2_error': float(np.std(test_l2_errors)),
                        'mean_success_rate': float(np.mean(test_success_rates)),
                        'mean_spl': float(np.mean(test_spls)),
                        'num_batches': len(test_l2_errors)
                    }
                    
                    all_test_results.append(run_results)
                    
                    logger.info(f"   L2è¯¯å·®: {run_results['mean_l2_error']:.4f} Â± {run_results['std_l2_error']:.4f}m")
                    logger.info(f"   æˆåŠŸç‡: {run_results['mean_success_rate']:.4f}")
                    logger.info(f"   SPL: {run_results['mean_spl']:.4f}")
                    logger.info(f"   æ‰¹æ¬¡æ•°: {run_results['num_batches']}")
        
        return all_test_results
    
    def analyze_consistency(self, test_results):
        """åˆ†æä¸€è‡´æ€§ç»“æœ"""
        if not test_results:
            logger.error("âŒ æ²¡æœ‰æµ‹è¯•ç»“æœå¯åˆ†æ")
            return
        
        logger.info("\n" + "="*80)
        logger.info("ğŸ“Š L2è¯¯å·®ä¸€è‡´æ€§åˆ†æ")
        logger.info("="*80)
        
        l2_errors = [r['mean_l2_error'] for r in test_results]
        success_rates = [r['mean_success_rate'] for r in test_results]
        spls = [r['mean_spl'] for r in test_results]
        
        logger.info(f"ğŸ“ˆ æµ‹è¯•æ¬¡æ•°: {len(test_results)}")
        logger.info(f"ğŸ“ˆ æ¯æ¬¡æµ‹è¯•æ‰¹æ¬¡æ•°: {test_results[0]['num_batches']}")
        
        logger.info(f"\nğŸ¯ L2è¯¯å·®ç»Ÿè®¡:")
        logger.info(f"   å„æ¬¡ç»“æœ: {[f'{l2:.4f}' for l2 in l2_errors]}")
        logger.info(f"   å¹³å‡å€¼: {np.mean(l2_errors):.4f}m")
        logger.info(f"   æ ‡å‡†å·®: {np.std(l2_errors):.6f}m")
        logger.info(f"   æœ€å°å€¼: {np.min(l2_errors):.4f}m")
        logger.info(f"   æœ€å¤§å€¼: {np.max(l2_errors):.4f}m")
        logger.info(f"   å˜åŒ–èŒƒå›´: {np.max(l2_errors) - np.min(l2_errors):.6f}m")
        
        logger.info(f"\nğŸ“Š æˆåŠŸç‡ç»Ÿè®¡:")
        logger.info(f"   å„æ¬¡ç»“æœ: {[f'{sr:.4f}' for sr in success_rates]}")
        logger.info(f"   æ ‡å‡†å·®: {np.std(success_rates):.6f}")
        
        logger.info(f"\nğŸ† SPLç»Ÿè®¡:")
        logger.info(f"   å„æ¬¡ç»“æœ: {[f'{spl:.4f}' for spl in spls]}")
        logger.info(f"   æ ‡å‡†å·®: {np.std(spls):.6f}")
        
        # ä¸€è‡´æ€§åˆ¤æ–­
        l2_std = np.std(l2_errors)
        if l2_std < 1e-6:
            logger.info(f"\nâœ… ç»“æœé«˜åº¦ä¸€è‡´ï¼L2è¯¯å·®æ ‡å‡†å·® < 1e-6")
        elif l2_std < 1e-4:
            logger.info(f"\nâœ… ç»“æœåŸºæœ¬ä¸€è‡´ï¼ŒL2è¯¯å·®æ ‡å‡†å·® < 1e-4")
        elif l2_std < 1e-2:
            logger.info(f"\nâš ï¸ ç»“æœæœ‰å°å¹…æ³¢åŠ¨ï¼ŒL2è¯¯å·®æ ‡å‡†å·® < 1e-2")
        else:
            logger.info(f"\nâŒ ç»“æœä¸å¤Ÿä¸€è‡´ï¼ŒL2è¯¯å·®æ ‡å‡†å·®è¾ƒå¤§: {l2_std:.6f}")
        
        logger.info("="*80)
    
    def run_test(self, num_tests=5):
        """è¿è¡Œæµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹BEVBertæ¨¡å‹L2è¯¯å·®ä¸€è‡´æ€§æµ‹è¯•...")
        
        # 1. åˆ›å»ºæ¨¡å‹
        if not self.create_model():
            logger.error("âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥")
            return False
        
        # 2. åŠ è½½æµ‹è¯•é›†
        test_episodes = self.load_test_dataset()
        if test_episodes is None:
            logger.error("âŒ æµ‹è¯•é›†åŠ è½½å¤±è´¥")
            return False
        
        # 3. è¿è¡Œä¸€è‡´æ€§æµ‹è¯•
        test_results = self.test_l2_consistency(test_episodes, num_tests)
        
        # 4. åˆ†æç»“æœ
        self.analyze_consistency(test_results)
        
        # 5. ä¿å­˜ç»“æœ
        try:
            results_file = Path("/data/yinxy/etpnav_training_data/bevbert_results/l2_consistency_test.json")
            results_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(results_file, 'w') as f:
                json.dump({
                    'model_path': str(self.checkpoint_path),
                    'test_results': test_results,
                    'summary': {
                        'num_tests': len(test_results),
                        'mean_l2_error': float(np.mean([r['mean_l2_error'] for r in test_results])),
                        'l2_std': float(np.std([r['mean_l2_error'] for r in test_results])),
                        'is_consistent': bool(np.std([r['mean_l2_error'] for r in test_results]) < 1e-4)
                    }
                }, f, indent=2)
            
            logger.info(f"ğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        except Exception as e:
            logger.warning(f"âš ï¸ ä¿å­˜ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
            logger.info("ğŸ’¡ ä½†æµ‹è¯•å·²æˆåŠŸå®Œæˆï¼Œç»“æœæ˜¾ç¤ºåœ¨ä¸Šæ–¹")
        
        return True

def main():
    parser = argparse.ArgumentParser(description='BEVBertæ¨¡å‹L2è¯¯å·®ä¸€è‡´æ€§æµ‹è¯•')
    parser.add_argument('--checkpoint', type=str, required=True, 
                        help='è®­ç»ƒå¥½çš„æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--num_tests', type=int, default=5,
                        help='æµ‹è¯•æ¬¡æ•° (é»˜è®¤5æ¬¡)')
    
    args = parser.parse_args()
    
    logger.info("ğŸ§ª BEVBertæ¨¡å‹L2è¯¯å·®ä¸€è‡´æ€§æµ‹è¯•")
    logger.info(f"   æ¨¡å‹è·¯å¾„: {args.checkpoint}")
    logger.info(f"   æµ‹è¯•æ¬¡æ•°: {args.num_tests}")
    
    tester = BEVBertTester(args.checkpoint)
    success = tester.run_test(args.num_tests)
    
    if success:
        logger.info("ğŸ‰ ä¸€è‡´æ€§æµ‹è¯•å®Œæˆï¼")
    else:
        logger.error("âŒ ä¸€è‡´æ€§æµ‹è¯•å¤±è´¥")
        sys.exit(1)

if __name__ == "__main__":
    main()